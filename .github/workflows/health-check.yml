name: NFL Scraper - Health Check & Monitoring

on:
  # Ejecutar cada 2 horas para monitoreo
  schedule:
    - cron: '15 */2 * * *'  # A los 15 minutos de cada 2 horas
  
  # Permitir ejecuciÃ³n manual
  workflow_dispatch:

env:
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
  GITHUB_ACTIONS: true

jobs:
  health-check:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: ğŸ“¥ Checkout repository
      uses: actions/checkout@v4
      
    - name: ğŸ Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install supabase python-dotenv
        
    - name: ğŸ” Database Health Check
      run: |
        echo "ğŸ¥ HEALTH CHECK - Sistema NFL Fantasy Scraper"
        echo "=============================================="
        echo "â° Timestamp: $(date)"
        
        # Ejecutar verificaciÃ³n bÃ¡sica de sistema
        python -c "
        import os
        from supabase import create_client
        from datetime import datetime
        
        print('ğŸ”Œ Verificando conexiÃ³n a Supabase...')
        url = os.getenv('SUPABASE_URL')
        key = os.getenv('SUPABASE_KEY')
        
        if not url or not key:
            print('âŒ Variables de entorno no configuradas')
            exit(1)
        
        try:
            supabase = create_client(url, key)
            
            # Verificar tabla
            response = supabase.table('nfl_fantasy_trends').select('scraped_at').limit(1).execute()
            print('âœ… ConexiÃ³n a BD: OK')
            
            # Verificar registros recientes
            recent_response = supabase.table('nfl_fantasy_trends').select('scraped_at').order('scraped_at', desc=True).limit(5).execute()
            
            if recent_response.data:
                print(f'ğŸ“Š Registros en BD: {len(recent_response.data)} encontrados')
                latest = recent_response.data[0]['scraped_at']
                print(f'ğŸ“… Ãšltimo registro: {latest[:19]}')
                
                # Verificar si hay actividad reciente (Ãºltimas 2 horas)
                from datetime import datetime, timedelta, timezone
                latest_dt = datetime.fromisoformat(latest.replace('Z', '+00:00'))
                now = datetime.now(timezone.utc)
                diff = now - latest_dt
                
                if diff.total_seconds() < 7200:  # 2 horas
                    print(f'âœ… Sistema activo: Ãºltimo scraping hace {int(diff.total_seconds()/60)} minutos')
                else:
                    print(f'âš ï¸ Posible problema: Ãºltimo scraping hace {int(diff.total_seconds()/3600)} horas')
            else:
                print('âš ï¸ No hay registros en la BD')
                
        except Exception as e:
            print(f'âŒ Error en health check: {e}')
            exit(1)
        "
        
    - name: ğŸ“ˆ Sistema Status Summary
      if: always()
      run: |
        echo "=============================================="
        echo "ğŸ“Š RESUMEN DEL SISTEMA:"
        echo "  â€¢ Scraper principal: Cada 30 minutos"
        echo "  â€¢ Health check: Cada 2 horas"
        echo "  â€¢ Modo: AutomÃ¡tico 24/7"
        echo "  â€¢ Anti-duplicados: Activo"
        echo "  â€¢ DetecciÃ³n de semanas: Activa"
        echo "=============================================="
