# ğŸ¤– CONFIGURACIÃ“N GITHUB ACTIONS - NFL Fantasy Scraper 24/7

## ğŸ¯ ConfiguraciÃ³n AutomÃ¡tica Cada 30 Minutos

### ğŸ“‹ Resumen de Workflows

1. **ğŸˆ Scraper Principal**: `nfl-scraper-30min.yml`
   - **Frecuencia**: Cada 30 minutos, 24/7
   - **Comando**: `python scrapper.py --auto`
   - **Protecciones**: MÃ¡ximas anti-duplicados
   - **Timeout**: 25 minutos (antes de la siguiente ejecuciÃ³n)

2. **ğŸ” Health Check**: `health-check.yml`
   - **Frecuencia**: Cada 2 horas
   - **FunciÃ³n**: Monitoreo de sistema y BD
   - **Verificaciones**: ConexiÃ³n, registros recientes, actividad

3. **ğŸ’¾ Backup Scraper**: `nfl-scraper-hourly.yml`
   - **Frecuencia**: Cada 4 horas (backup)
   - **FunciÃ³n**: Respaldo del sistema principal
   - **Horario**: A los 45 minutos de cada 4 horas

## â° Horarios de EjecuciÃ³n (UTC)

### Scraper Principal (cada 30 minutos):
```
00:00, 00:30, 01:00, 01:30, 02:00, 02:30, ...
...continÃºa las 24 horas...
22:00, 22:30, 23:00, 23:30
```

### Health Check (cada 2 horas):
```
00:15, 02:15, 04:15, 06:15, 08:15, 10:15
12:15, 14:15, 16:15, 18:15, 20:15, 22:15
```

### Backup (cada 4 horas):
```
00:45, 04:45, 08:45, 12:45, 16:45, 20:45
```

## ğŸ”§ ConfiguraciÃ³n Requerida

### 1. Secrets de GitHub
Configura estos secrets en tu repositorio:
- `SUPABASE_URL`: URL de tu proyecto Supabase
- `SUPABASE_KEY`: Clave anÃ³nima de Supabase

### 2. Campo Semana en Supabase
Ejecuta este SQL en tu base de datos:
```sql
ALTER TABLE nfl_fantasy_trends ADD COLUMN semana INTEGER DEFAULT 1;
CREATE INDEX idx_nfl_fantasy_trends_semana ON nfl_fantasy_trends(semana);
```

## ğŸª Funcionalidades del Sistema

### âœ… DetecciÃ³n AutomÃ¡tica de Semana NFL
- Web scraping de fantasy.nfl.com
- CÃ¡lculo por fecha como fallback
- Comparaciones inteligentes por semana

### âœ… Sistema Anti-Duplicados
- ComparaciÃ³n individual por jugador
- DetecciÃ³n de cambios reales Ãºnicamente
- Validaciones pre y post-scraping

### âœ… Monitoreo Continuo
- Health checks automÃ¡ticos
- Logs detallados en cada ejecuciÃ³n
- Sistema de backup redundante

## ğŸ“Š Volumen de Ejecuciones

### Por DÃ­a:
- **Scraper Principal**: 48 ejecuciones (cada 30 min)
- **Health Check**: 12 ejecuciones (cada 2 horas)
- **Backup**: 6 ejecuciones (cada 4 horas)
- **Total**: 66 ejecuciones por dÃ­a

### Por Semana:
- **Scraper Principal**: 336 ejecuciones
- **Total General**: 462 ejecuciones

### Por Mes:
- **Scraper Principal**: ~1,440 ejecuciones
- **Total General**: ~1,980 ejecuciones

## ğŸ” Monitoring y Debugging

### Logs en GitHub Actions:
- Cada workflow genera logs detallados
- Timestamps de inicio y fin
- EstadÃ­sticas de registros procesados
- DetecciÃ³n de errores automÃ¡tica

### VerificaciÃ³n Manual:
```bash
# Probar detecciÃ³n de semana
python scrapper.py --test-week

# Probar comparaciÃ³n individual
python scrapper.py --test-individual

# Ejecutar modo automÃ¡tico local
python scrapper.py --auto
```

## ğŸš¨ Manejo de Errores

### Timeouts:
- Cada job tiene timeout de 25 minutos
- Permite completar antes de la siguiente ejecuciÃ³n

### Fallos de Red:
- Sistema de retry implÃ­cito en la siguiente ejecuciÃ³n
- Backup cada 4 horas como redundancia

### Fallos de BD:
- Health check detecta problemas de conexiÃ³n
- Logs detallados para debugging

## ğŸ’¡ Optimizaciones Implementadas

### 1. **Cache de Dependencias**
- Python dependencies cacheadas
- InstalaciÃ³n mÃ¡s rÃ¡pida en ejecuciones subsecuentes

### 2. **InstalaciÃ³n Chrome Optimizada**
- Chrome estable instalado una vez por job
- ConfiguraciÃ³n headless para GitHub Actions

### 3. **Modo Auto EspecÃ­fico**
- Sin archivos locales en GitHub Actions
- Solo operaciones de BD crÃ­ticas
- Validaciones de seguridad mÃ¡ximas

## ğŸ¯ Resultados Esperados

### Primera Semana:
- Sistema detectarÃ¡ semana NFL actual
- InsertarÃ¡ baseline de jugadores
- EstablecerÃ¡ rutina de monitoreo

### OperaciÃ³n Normal:
- Solo cambios reales insertados
- 0-5% de jugadores con cambios tÃ­picos
- Transiciones de semana manejadas automÃ¡ticamente

### MÃ©tricas de Ã‰xito:
- **Uptime**: 99%+ (48 intentos por dÃ­a)
- **Duplicados**: 0% (sistema anti-duplicados)
- **Cambios detectados**: 0-10% por ejecuciÃ³n
- **Tiempo de ejecuciÃ³n**: 5-15 minutos por scraping

---

## ğŸš€ Estado: LISTO PARA PRODUCCIÃ“N

El sistema estÃ¡ completamente configurado para operar 24/7 con:
- âœ… DetecciÃ³n automÃ¡tica de semanas NFL
- âœ… Protecciones anti-duplicados mÃ¡ximas  
- âœ… Monitoreo continuo y health checks
- âœ… Sistema de backup redundante
- âœ… Logging y debugging completo

**PrÃ³ximo paso**: Commit y push para activar los workflows automÃ¡ticos.
