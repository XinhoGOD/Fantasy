# ğŸˆ NFL Fantasy Scraper - ConfiguraciÃ³n GitHub Actions 24/7

## ğŸ“‹ RESUMEN DE CONFIGURACIÃ“N COMPLETADA

### âœ… WORKFLOWS CONFIGURADOS

#### 1. **Workflow Principal: `nfl-scraper-30min.yml`**
- **Frecuencia**: â° Cada 30 minutos, 24 horas al dÃ­a
- **ProgramaciÃ³n**: `0,30 * * * *` (minutos 0 y 30 de cada hora)
- **Ejecuciones por dÃ­a**: 48 ejecuciones automÃ¡ticas
- **Modo**: `python scrapper.py --auto` (mÃ¡ximas protecciones anti-duplicados)
- **CaracterÃ­sticas**:
  - âœ… DetecciÃ³n automÃ¡tica de semana NFL
  - âœ… ComparaciÃ³n individual por jugador (evita duplicados)
  - âœ… InstalaciÃ³n optimizada de Chrome y dependencias
  - âœ… Timeout de 25 minutos (seguridad antes de prÃ³xima ejecuciÃ³n)
  - âœ… Logging detallado y notificaciones de error
  - âœ… RecuperaciÃ³n automÃ¡tica en caso de fallos

#### 2. **Workflow de Monitoreo: `health-check-advanced.yml`**
- **Frecuencia**: â° Cada 2 horas 
- **FunciÃ³n**: Verificar salud del sistema y estadÃ­sticas de BD
- **CaracterÃ­sticas**:
  - âœ… VerificaciÃ³n de conexiÃ³n Supabase
  - âœ… Conteo de registros y sesiones
  - âœ… AnÃ¡lisis de actividad reciente
  - âœ… VerificaciÃ³n del campo `semana`
  - âœ… AnÃ¡lisis detallado bajo demanda

#### 3. **Workflow Existente: `health-check.yml`**
- Mantener como backup o remover segÃºn preferencia

### âš™ï¸ CONFIGURACIÃ“N TÃ‰CNICA

#### Variables de Entorno Requeridas (GitHub Secrets)
```
SUPABASE_URL=tu_url_de_supabase
SUPABASE_KEY=tu_clave_anonima_de_supabase
```

#### Dependencias Optimizadas (`requirements.txt`)
```
selenium==4.15.2        # Web scraping
beautifulsoup4==4.12.2  # HTML parsing  
supabase==1.2.0         # Base de datos
python-dotenv==1.0.0    # Variables de entorno
httpx[http2]==0.24.1    # HTTP client para Supabase
```

### ğŸš€ FUNCIONAMIENTO AUTOMÃTICO

#### Flujo de EjecuciÃ³n (Cada 30 Minutos)
```
1. GitHub Actions dispara workflow
2. Instala Chrome y dependencias Python
3. Ejecuta: python scrapper.py --auto
4. Sistema detecta semana NFL automÃ¡ticamente
5. Compara jugadores con registros individuales previos
6. Inserta solo cambios reales (anti-duplicados)
7. Logs detallados del resultado
8. Espera 30 minutos y repite
```

#### Protecciones Implementadas
- âœ… **Anti-duplicados mÃ¡ximos**: ComparaciÃ³n individual por jugador
- âœ… **DetecciÃ³n inteligente**: Solo inserta cambios reales
- âœ… **Timeout de seguridad**: 25 minutos mÃ¡ximo por ejecuciÃ³n
- âœ… **RecuperaciÃ³n automÃ¡tica**: ContinÃºa tras errores
- âœ… **Logging completo**: Trazabilidad total del proceso
- âœ… **VerificaciÃ³n de integridad**: Valida resultados antes/despuÃ©s

### ğŸ“Š CRONOGRAMA DE EJECUCIÃ“N

#### Ejecuciones Diarias (Ejemplo)
```
00:00 UTC - Scraping automÃ¡tico
00:30 UTC - Scraping automÃ¡tico  
01:00 UTC - Scraping automÃ¡tico
01:30 UTC - Scraping automÃ¡tico
02:00 UTC - Scraping automÃ¡tico + Health Check
02:30 UTC - Scraping automÃ¡tico
03:00 UTC - Scraping automÃ¡tico
03:30 UTC - Scraping automÃ¡tico
04:00 UTC - Scraping automÃ¡tico + Health Check
...y asÃ­ 24 horas al dÃ­a, 7 dÃ­as a la semana
```

#### EstadÃ­sticas Esperadas
- **Ejecuciones diarias**: 48 scrapings + 12 health checks
- **Registros por semana**: Variable segÃºn cambios NFL
- **Uptime esperado**: 99%+ (GitHub Actions es muy confiable)

### ğŸ¯ COMANDOS MANUALES DISPONIBLES

#### Desde GitHub Actions UI:
```
1. Ir a Actions â†’ "NFL Fantasy Scraper 24/7"
2. Click "Run workflow" 
3. Opciones disponibles:
   - "Normal execution" (modo automÃ¡tico)
   - "Test mode" (prueba sin insertar datos)
   - "Week detection test" (solo probar detecciÃ³n de semana)
```

#### Desde lÃ­nea de comandos (local):
```bash
python scrapper.py --auto              # Modo automÃ¡tico (GitHub Actions)
python scrapper.py --test             # Prueba general
python scrapper.py --test-week        # Prueba detecciÃ³n de semana
python scrapper.py --test-individual  # Prueba comparaciÃ³n individual
python scrapper.py --help            # Ver todas las opciones
```

### âœ… CHECKLIST DE VALIDACIÃ“N

#### Antes de Activar ProducciÃ³n:
- [ ] Secrets configurados en GitHub (SUPABASE_URL, SUPABASE_KEY)
- [ ] Campo `semana` agregado a tabla Supabase âœ…
- [ ] Workflow `nfl-scraper-30min.yml` commitado âœ…
- [ ] `requirements.txt` optimizado âœ…
- [ ] Prueba manual exitosa: `python scrapper.py --auto`

#### DespuÃ©s de Activar:
- [ ] Verificar primera ejecuciÃ³n automÃ¡tica en Actions
- [ ] Comprobar que se insertan datos en Supabase
- [ ] Validar que no hay duplicados masivos
- [ ] Monitorear logs durante primeras horas

### ğŸ”„ MANTENIMIENTO

#### Automatizado (No Requiere IntervenciÃ³n):
- âœ… EjecuciÃ³n cada 30 minutos
- âœ… RecuperaciÃ³n de errores temporales
- âœ… DetecciÃ³n automÃ¡tica de semanas NFL
- âœ… Monitoreo de salud del sistema

#### Manual (Ocasional):
- ğŸ” Revisar logs si hay errores persistentes
- ğŸ“Š Verificar estadÃ­sticas de base de datos
- ğŸ”§ Actualizar dependencias si es necesario
- ğŸˆ Ajustar lÃ³gica si la NFL cambia formato de datos

### ğŸ‰ RESULTADO FINAL

**El sistema estÃ¡ completamente configurado para ejecutarse automÃ¡ticamente cada 30 minutos, las 24 horas del dÃ­a, con mÃ¡ximas protecciones contra duplicados y detecciÃ³n inteligente de cambios.**

**PrÃ³ximo paso**: Activar el workflow haciendo commit/push de los cambios a GitHub.
